{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb4f22e",
   "metadata": {},
   "source": [
    "**Data Deduplication Rationale**\n",
    "\n",
    "As we collected images of climbing holds from multiple data sources on the same climbing site, there is a significant risk of having duplicate images in our dataset. Duplicates can bias the training process by causing the model to overfit, distort the evaluation metrics, and reduce its ability to generalize to new data. To ensure the quality and reliability of our model, we will implement a deduplication step to remove identical or highly similar images before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa48a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_DIR = os.getcwd().split('notebooks')[0]\n",
    "\n",
    "IMAGES_DIR = [\n",
    "    os.path.join(PROJECT_DIR, 'data/', data_dir, sub_dir, 'images')\n",
    "    for data_dir in ['block/', 'montagne/']\n",
    "    for sub_dir in ['train/', 'valid/', 'test/']\n",
    "]\n",
    "\n",
    "images_path = [\n",
    "    os.path.join(images_dir, image)\n",
    "    for images_dir in IMAGES_DIR\n",
    "    for image in os.listdir(images_dir)\n",
    "    if image.endswith('.jpg')\n",
    "]\n",
    "\n",
    "assert len(images_path) == 9427, f'Expected 9427 images, but found {len(images_path)} images.'  # checked manually to get the number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f3d08",
   "metadata": {},
   "source": [
    "**Duplicate Detection Method**\n",
    "\n",
    "To detect duplicate images, we will compare the MD5 hash values of each image. MD5 is a widely used hashing algorithm that generates a unique fingerprint for each file. By using MD5, we can efficiently and reliably identify exact duplicates, since identical images will have the same hash value. This method is fast and well-suited for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6617038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def file_hash(path):\n",
    "    \"\"\"Calcule le hash MD5 d’un fichier.\"\"\"\n",
    "    h = hashlib.md5()\n",
    "    with open(path, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7ea1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"duplicates = {}\\nfor dir in ['block/', 'montagne/']:\\n    for sub_dir in ['train/', 'valid/', 'test/']:\\n        dir_path = os.path.join(PROJECT_DIR, 'data/', dir, sub_dir, 'images')\\n        duplicates[dir_path] = find_duplicates(dir_path)\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "PROJECT_DIR = os.getcwd().split('notebooks')[0]\n",
    "\n",
    "def file_hash(path):\n",
    "    \"\"\"Calcule le hash MD5 d’un fichier.\"\"\"\n",
    "    h = hashlib.md5()\n",
    "    with open(path, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def find_duplicates(folder):\n",
    "    hashes = {}\n",
    "    duplicates = []\n",
    "    \n",
    "    for root, _, files in os.walk(folder):\n",
    "        for fname in files:\n",
    "            if fname.lower().endswith('.jpg'):\n",
    "                fpath = os.path.join(root, fname)\n",
    "                h = file_hash(fpath)\n",
    "                if h in hashes:\n",
    "                    duplicates.append((fpath, hashes[h]))\n",
    "                else:\n",
    "                    hashes[h] = fpath\n",
    "\n",
    "    return duplicates\n",
    "\n",
    "\"\"\"duplicates = {}\n",
    "for data_dir in ['block/', 'montagne/']:\n",
    "    for sub_dir in ['train/', 'valid/', 'test/']:\n",
    "        dir_path = os.path.join(PROJECT_DIR, 'data/', data_dir, sub_dir, 'images')\n",
    "        duplicates[dir_path] = find_duplicates(dir_path)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d30abde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/alessandroarensberg/Documents/summit-seeker/data/block/train/images',\n",
       " '/Users/alessandroarensberg/Documents/summit-seeker/data/block/valid/images',\n",
       " '/Users/alessandroarensberg/Documents/summit-seeker/data/block/test/images',\n",
       " '/Users/alessandroarensberg/Documents/summit-seeker/data/montagne/train/images',\n",
       " '/Users/alessandroarensberg/Documents/summit-seeker/data/montagne/valid/images',\n",
       " '/Users/alessandroarensberg/Documents/summit-seeker/data/montagne/test/images']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[os.path.join(PROJECT_DIR, 'data/', data_dir, sub_dir, 'images') for data_dir in ['block/', 'montagne/'] for sub_dir in ['train/', 'valid/', 'test/']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
