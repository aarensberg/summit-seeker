{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9430e84b",
   "metadata": {},
   "source": [
    "**Data duplication rationale**\n",
    "\n",
    "As we collected images of climbing holds from multiple data sources on the same climbing site, there is a significant risk of having duplicate images in our dataset. Duplicates can bias the training process by causing the model to overfit, distort the evaluation metrics, and reduce its ability to generalize to new data. To ensure the quality and reliability of our model, we will implement a deduplication step to remove identical or highly similar images before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74f3bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9427,\n",
       " ['/Users/alessandroarensberg/Documents/summit-seeker/data/boulder/train/images/yellow_v2_cave_feb21_frame_sec_10_jpg.rf.d39c54db62f22b16fa6554ace0b92e98.jpg',\n",
       "  '/Users/alessandroarensberg/Documents/summit-seeker/data/boulder/train/images/img6_jpg.rf.46f24b188d525ec02e0cae1aee5d8acd.jpg',\n",
       "  '/Users/alessandroarensberg/Documents/summit-seeker/data/boulder/train/images/wall_img_16_jpg.rf.6cf49f5119aaa43993ae718e3056654b.jpg',\n",
       "  '/Users/alessandroarensberg/Documents/summit-seeker/data/boulder/train/images/yellow_v2_cave_feb22_frame_sec_002_jpg.rf.588c3fc9f1addb7eee0dbe17480007b1.jpg',\n",
       "  '/Users/alessandroarensberg/Documents/summit-seeker/data/boulder/train/images/34-3_jpg.rf.7c1029353ec68c88fe1d617affa199fb.jpg'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_PATH = os.getcwd().split('notebooks')[0]\n",
    "\n",
    "IMAGES_DIR_PATHS = [\n",
    "    os.path.join(PROJECT_PATH, 'data/', data_dir, sub_dir, 'images')\n",
    "    for data_dir in ['boulder/', 'mountain/']\n",
    "    for sub_dir in ['train/', 'valid/', 'test/']\n",
    "]\n",
    "\n",
    "IMAGE_PATHS = [\n",
    "    os.path.join(dir_path, file_name)\n",
    "    for dir_path in IMAGES_DIR_PATHS\n",
    "    for file_name in os.listdir(dir_path)\n",
    "    if file_name.endswith('.jpg')\n",
    "]\n",
    "\n",
    "assert len(IMAGE_PATHS) == 9427, f'Expected 9427 images, but found {len(IMAGE_PATHS)} images.'  # checked manually to get the number\n",
    "len(IMAGE_PATHS), IMAGE_PATHS[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49070bd9",
   "metadata": {},
   "source": [
    "**Duplicate detection method**\n",
    "\n",
    "To detect duplicate images, we will compare the MD5 hash values of each image. MD5 is a widely used hashing algorithm that generates a unique fingerprint for each file. By using MD5, we can efficiently and reliably identify exact duplicates, since identical images will have the same hash value. This method is fast and well-suited for large datasets.\n",
    "\n",
    "To evaluate the effectiveness of hash-based deduplication, I manually selected images that look exactly the same but have different file names. I will then compare their hash values. This step will help determine if the hash method is sufficient for identifying all duplicates, or if some visually similar images require a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f43a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7df45aeed145811d58f7ed2e6ab20858',\n",
       " '7df45aeed145811d58f7ed2e6ab20858',\n",
       " 'd89c1c20f13afc2518e141f11f90dd95',\n",
       " 'd89c1c20f13afc2518e141f11f90dd95']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "def file_hash(file_path: str) -> str:\n",
    "    \"\"\"Calculate the MD5 hash of a file.\"\"\"\n",
    "    h = md5()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "[file_hash(os.path.join('/Users/alessandroarensberg/Documents/summit-seeker/data/boulder/test/images/', file_name))\n",
    " for file_name in [\n",
    "     'ArticleImageHandler_jpeg_jpg.rf.2fe6836468df96f3757cda82d79de585.jpg',\n",
    "     'ArticleImageHandler_jpeg_jpg.rf.7f7778d73a1cf7151e20834b4e9c1cb2.jpg',\n",
    "     'ArticleImageHandler_jpeg_jpg.rf.ae70d55cfee35c218ba3b99d2cc98646.jpg',\n",
    "     'ArticleImageHandler_jpeg_jpg.rf.c63f4489daf9f9f511b7421d01db8e95.jpg'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0e0ec",
   "metadata": {},
   "source": [
    "**Limitations of binary hash comparison**\n",
    "\n",
    "After comparing four visually similar images, I noticed that not all of them share the same hash value. This happens because the MD5 hash checks the binary content of the files, not their visual appearance. Even small differences in file encoding or compression can result in different hashes, even if the images look almost identical.\n",
    "\n",
    "To address this, I will use a perceptual hash (such as the `imagehash`⁠ library). A perceptual hash summarizes the visual features of an image, rather than its exact binary content. This approach will allow me to compare images based on their visual similarity and improve the accuracy of the deduplication process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505304f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.Image import open\n",
    "from imagehash import phash\n",
    "\n",
    "visually_similar = [phash(\n",
    "    open(\n",
    "        os.path.join('/Users/alessandroarensberg/Documents/summit-seeker/data/boulder/test/images/', file_name)))\n",
    "        for file_name in [\n",
    "            'ArticleImageHandler_jpeg_jpg.rf.2fe6836468df96f3757cda82d79de585.jpg',\n",
    "            'ArticleImageHandler_jpeg_jpg.rf.7f7778d73a1cf7151e20834b4e9c1cb2.jpg',\n",
    "            'ArticleImageHandler_jpeg_jpg.rf.ae70d55cfee35c218ba3b99d2cc98646.jpg',\n",
    "            'ArticleImageHandler_jpeg_jpg.rf.c63f4489daf9f9f511b7421d01db8e95.jpg'\n",
    "        ]\n",
    "]\n",
    "\n",
    "not_visually_similar = [phash(\n",
    "    open(\n",
    "        os.path.join('/Users/alessandroarensberg/Documents/summit-seeker/data/boulder/test/images/', file_name)))\n",
    "        for file_name in [\n",
    "            'AF1QipMHSzeW9a3RBngF-kjg-s9qVsoXgyhNGw6BYzV8-w406-h318-k-no_jpeg_jpg.rf.d7b7ca9416109dc17a31d7f55684a32f.jpg',\n",
    "            'AF1QipN-svP3tQtZP4mILTpB1G0B3FFXGYrasBg6YHSi-s1024_jpeg_jpg.rf.1b8616ab82bb3bef2e73045336221457.jpg',\n",
    "            'AF1QipP1IcXZ_ySbNmSOzJ24yxOjVLe5VAIsa8oNif9e-w406-h721-k-no_jpeg_jpg.rf.ba8e81ace008620c80842b1d73acfc70.jpg',\n",
    "            'AF1QipP4tRtE_sz_okNuAp7_kKMf86QMxVzsr8GC1QS9-s512_jpeg_jpg.rf.49b0efdb7cd460f62cb4b04263e7d84d.jpg'\n",
    "        ]\n",
    "]\n",
    "\n",
    "assert all(visually_similar[0] == img_hash for img_hash in visually_similar[1:]) == True, 'Some images are not identity as visually similar, but they should be'\n",
    "assert all(not_visually_similar[i] != not_visually_similar[j]\n",
    "           for i in range(len(not_visually_similar))\n",
    "           for j in range(i + 1, len(not_visually_similar))) == True, 'Some images are identity as visually similar, but they should not be'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4baaf",
   "metadata": {},
   "source": [
    "**Perceptual hash results**\n",
    "\n",
    "The tests with the perceptual hash are promising. When I compare four visually similar images, they all have the same hash value. On the other hand, four visually different images each have a unique hash. This confirms that the perceptual hash is effective at capturing visual similarity and distinguishing between different images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd82cb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1733,\n",
       " ['/Users/alessandroarensberg/Documents/summit-seeker/data/boulder/valid/images/wall_img_18_jpg.rf.6b8aa3c81004f3e7cbf5c8fde179dee1.jpg',\n",
       "  '/Users/alessandroarensberg/Documents/summit-seeker/data/boulder/train/images/IMG_4274_JPG_jpg.rf.5ab2eb7a56fa6eb6ec06dac728a74d27.jpg',\n",
       "  '/Users/alessandroarensberg/Documents/summit-seeker/data/boulder/train/images/IMG_2520_jpeg_jpg.rf.1a7b9acfa489d1d5277c84c874e28f73.jpg',\n",
       "  '/Users/alessandroarensberg/Documents/summit-seeker/data/boulder/train/images/IMG_2520_jpeg_jpg.rf.3ccc7b202e82eb2122022e282cccfb2d.jpg',\n",
       "  '/Users/alessandroarensberg/Documents/summit-seeker/data/boulder/train/images/IMG_2520_jpeg_jpg.rf.7caa1072e993b7db2d8d5559b73e09f8.jpg'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_hash = {\n",
    "    path: phash(open(path))\n",
    "    for path in IMAGE_PATHS\n",
    "}\n",
    "\n",
    "assert len(images_hash) == 9427, f'Expected 9427 images, but found {len(images_hash)} images'  # checked manually to get the number\n",
    "assert None not in images_hash.values(), 'Some images have no hash'\n",
    "\n",
    "hash_to_paths = {\n",
    "    img_hash: [p for p, h in images_hash.items() if h == img_hash]\n",
    "    for img_hash in set(images_hash.values())\n",
    "    if len([p for p, h in images_hash.items() if h == img_hash]) > 1  # only hashes corresponding to several images are retained\n",
    "}\n",
    "\n",
    "FILES_TO_REMOVE = [path\n",
    " for duplicated in hash_to_paths.values()  # browse each sublist of duplicated images\n",
    " for path in duplicated[1:]]  # keep paths of all but one the first image (the one to keep)\n",
    "\n",
    "len(FILES_TO_REMOVE), FILES_TO_REMOVE[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8423e21",
   "metadata": {},
   "source": [
    "**Image and label consistency check**\n",
    "\n",
    "Before removing the duplicate files, we will check that the filenames of the images and their corresponding labels match correctly. After deleting the duplicate files, we will repeat this check to ensure that the associated labels have also been properly removed. This step helps maintain consistency between the images and their labels throughout the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a63752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ /boulder/train\n",
      "✅ /boulder/valid\n",
      "✅ /boulder/test\n",
      "✅ /mountain/train\n",
      "✅ /mountain/valid\n",
      "✅ /mountain/test\n"
     ]
    }
   ],
   "source": [
    "def image_label_consistency(images_dir_paths: str, labels_dir_paths: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if image and label file names in all data directories match\n",
    "    \n",
    "    Args:\n",
    "        images_dir_paths (str): Paths to the directories containing images\n",
    "        labels_dir_paths (str): Paths to the directories containing labels\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all image and label file names match, False otherwise\n",
    "        list: List of directories where the file names do not match\n",
    "    \"\"\"\n",
    "    \n",
    "    problematic_dirs = []\n",
    "\n",
    "    for image_dir, label_dir in zip(images_dir_paths, labels_dir_paths):\n",
    "        image_files = [file.split('.jpg')[0] for file in os.listdir(image_dir) if file.endswith('.jpg')]\n",
    "        label_files = [file.split('.txt')[0] for file in os.listdir(label_dir) if file.endswith('.txt')]\n",
    "        \n",
    "        dir_name = image_dir.split('data')[1].split('/images')[0]\n",
    "        \n",
    "        if not set(image_files) == set(label_files):\n",
    "            problematic_dirs.append(dir_name)\n",
    "            print(f'❌ {dir_name}')\n",
    "        else:\n",
    "            print(f'✅ {dir_name}')\n",
    "\n",
    "    return (True, None) if not problematic_dirs else (False, problematic_dirs)\n",
    "\n",
    "LABELS_DIR_PATHS = [\n",
    "    os.path.join(PROJECT_PATH, 'data/', data_dir, sub_dir, 'labels')\n",
    "    for data_dir in ['boulder/', 'mountain/']\n",
    "    for sub_dir in ['train/', 'valid/', 'test/']\n",
    "]\n",
    "\n",
    "assert image_label_consistency(IMAGES_DIR_PATHS, LABELS_DIR_PATHS)[0], 'Some images and labels do not match'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f615d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ /boulder/train\n",
      "✅ /boulder/valid\n",
      "✅ /boulder/test\n",
      "✅ /mountain/train\n",
      "✅ /mountain/valid\n",
      "✅ /mountain/test\n"
     ]
    }
   ],
   "source": [
    "for file_name in FILES_TO_REMOVE:\n",
    "    os.remove(file_name)  # remove the image\n",
    "    os.remove(file_name.replace('images', 'labels').replace('.jpg', '.txt'))  # remove the corresponding label\n",
    "\n",
    "assert image_label_consistency(IMAGES_DIR_PATHS, LABELS_DIR_PATHS)[0], 'Deleting the files led to inconsistencies between images and labels'\n",
    "\n",
    "NEW_IMAGE_PATHS = [\n",
    "    os.path.join(dir_path, file_name)\n",
    "    for dir_path in IMAGES_DIR_PATHS\n",
    "    for file_name in os.listdir(dir_path)\n",
    "    if file_name.endswith('.jpg')\n",
    "]\n",
    "\n",
    "assert len(NEW_IMAGE_PATHS) == len(IMAGE_PATHS) - len(FILES_TO_REMOVE), f'Expected {len(IMAGE_PATHS) - len(FILES_TO_REMOVE)} images ; found {len(NEW_IMAGE_PATHS)}'\n",
    "len(NEW_IMAGE_PATHS), NEW_IMAGE_PATHS[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ddfd5",
   "metadata": {},
   "source": [
    "**Deduplication summary**\n",
    "\n",
    "We started with 9427 images in the dataset. After removing duplicates, we have 7694 images left. This means we found and removed 1733 duplicate images. The dataset is now cleaner and more reliable for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
